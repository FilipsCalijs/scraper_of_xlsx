import os
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

# === –ü–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–≥–æ .xlsx-—Ñ–∞–π–ª–∞ ===
xlsx_files = [f for f in os.listdir('.') if f.lower().endswith('.xlsx')]
if not xlsx_files:
    print("‚ùå –ù–µ—Ç .xlsx —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫–µ.")
    exit()

excel_file = xlsx_files[0]
print(f"üìÅ –ù–∞–π–¥–µ–Ω Excel-—Ñ–∞–π–ª: {excel_file}")

# === –§–∞–π–ª—ã –≤—ã–≤–æ–¥–∞ ===
all_domains_file = 'all_domains.txt'
working_file = 'domains_sus(403).txt'
problem_file = 'domains_problem.txt'
MAX_THREADS = 20
TIMEOUT = 3

print("‚öôÔ∏è –ù–∞—á–∏–Ω–∞—é —Ä–∞–±–æ—Ç—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–µ –≤—ã–∫–ª—é—á–∞–π—Ç–µ –∫–æ–º–ø—å—é—Ç–µ—Ä...")

# === –û—Ç–∫—Ä—ã—Ç–∏–µ Excel ===
xls = pd.ExcelFile(excel_file)
sheet_names = xls.sheet_names

seen_domains = set()
new_domains = []

for sheet_name in sheet_names:
    if sheet_name.lower() == 'uprankd':
        print(f"üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω –ª–∏—Å—Ç 'uprankd'. –ü—Ä–µ–∫—Ä–∞—â–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É.")
        break

    print(f"üìÑ –ß—Ç–µ–Ω–∏–µ –ª–∏—Å—Ç–∞: {sheet_name}")
    df = pd.read_excel(xls, sheet_name=sheet_name)

    if df.shape[1] < 2:
        continue

    col_b = df.iloc[:, 1]

    added = 0
    for val in col_b:
        if pd.isna(val):
            continue
        domain = str(val).strip()
        if not domain or domain in seen_domains:
            continue
        seen_domains.add(domain)
        new_domains.append(domain)
        added += 1

    print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ {added} –¥–æ–º–µ–Ω–æ–≤ –∏–∑ –ª–∏—Å—Ç–∞: {sheet_name}")

print(f"üî¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(new_domains)}")

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ ===
with open(all_domains_file, "w", encoding="utf-8") as f:
    for domain in new_domains:
        f.write(domain + "\n")

print(f"üìÑ –í—Å–µ –¥–æ–º–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {all_domains_file}")

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è URL-–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ ===
def generate_urls(domain):
    return [
        f"http://{domain}",
        f"http://www.{domain}",
        f"https://{domain}",
        f"https://www.{domain}"
    ]

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞ ===
def check_domain(domain):
    urls = generate_urls(domain)
    for url in urls:
        try:
            response = requests.head(url, timeout=TIMEOUT, allow_redirects=True)
            if response.status_code == 200:
                return ("ok200", f"{domain} - 200 ({url})")
            elif response.status_code == 403:
                return ("ok403", f"{domain}")
            else:
                return ("fail", f"{domain} - {response.status_code} ({url})")
        except Exception:
            continue
    return ("fail", f"{domain} - Error")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ all_domains.txt ===
with open(all_domains_file, "r", encoding="utf-8") as f:
    domains = [line.strip() for line in f if line.strip()]

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ ===
ok_200 = []
ok_403 = []
fail = []

with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
    future_map = {executor.submit(check_domain, domain): domain for domain in domains}

    for future in as_completed(future_map):
        status, result = future.result()
        if status == "ok200":
            ok_200.append(result)
        elif status == "ok403" and result:
            ok_403.append(result)
        elif status == "fail" and result:
            fail.append(result)

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
with open(working_file, "w", encoding="utf-8") as f_ok:
    for line in ok_403:
        f_ok.write(line + "\n")

with open(problem_file, "w", encoding="utf-8") as f_fail:
    for line in fail:
        f_fail.write(line + "\n")

# === –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ===
print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ ‚Äî –¥–æ–º–µ–Ω–æ–≤ —Å 200 —Å—Ç–∞—Ç—É—Å–æ–º: {len(ok_200)}")
print(f"üü° –°—Ç–∞—Ç—É—Å—ã 403 —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {working_file} ‚Äî {len(ok_403)}")
print(f"üî¥ –û—à–∏–±–∫–∏/–ø—Ä–æ–±–ª–µ–º—ã ‚Äî –≤: {problem_file} ‚Äî {len(fail)}")
